---
title: "Final Assignment Models Predictors"
author: "Eduard Romero / Carlos Bejarano / Juan Cepeda"
date: "2023-05-27"
output: pdf_document
header-includes:
  - \usepackage{xcolor}
  - \usepackage{titling}
  - \pretitle{\begin{center}\Large\color{blue}\textbf}
  - \posttitle{\end{center}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Problem definition.

Mobile robotics is a popular solution for exploration of hostile environments (such as toxic or radioactive
environments) where a direct human intervention is not possible. In this project it is asked that each team
implements a robotic explorer and simulates 3 different environments.

-   1.1 A total of 3 different environments needs to be simulated. Each environment needs to provide at least 3
conditions that can be sensed by the robot.
-   1.1.2 The robot needs to be able of moving from one environment to another.
-   1.1.3 Configuration of the environments (order) must be interchangeable.
-   1.1.4 The robot needs to acquire 3 or more sensor signals that can be use as predictors for supervised algorithms.

# 1.1 Controlled environments.
  
  - Cold Room.
  - Hot Room.
  - Toxic Room.

# 1.1.2 Robot characteristics.
  
  The main features of our robot are as follows:
  
  - MCU Arduino.
  - Micromotors DC.
  - PCB.
  - Battery LIPO 7.4V 300mAh.
  - Driver motor TB6612FNG.
  - Bluetooth HC06.
  
# 1.1.3 Configuration enviroments.

The configuration of our 3 environments will be placed in cascade form one after the other, where our robot will take 50 samples for each environment, each of them will be made of cardboard boxes and conditioned for each situation mentioned.

# 1.1.4 Configuration Sensors.
  
  Our robot has 3 analog sensors:
  
  - Sensor for air quality measurement MQ135.
  - Sensor Humidity DHT11.
  - Sensor Temperature LM35(DHT11).
  
  
# 2. Arduino code program.


[Code Source on repository GitHub](https://github.com/Eduard925/PredictorModels/blob/main/Code%20Arduino.cpp)



# 3. Methods for prediction.
  - 3.1 KNN without preprocessing:
  K-Nearest Neighbors (KNN) is a non-parametric classification algorithm. It makes predictions based on the majority class of the K nearest neighbors in the feature space. Without preprocessing, KNN uses the raw data as input, without any transformation or scaling.
  
  - 3.2 KNN with preprocessing:
  KNN can benefit from preprocessing techniques such as feature scaling, normalization, or dimensionality reduction. Preprocessing helps to improve the performance and accuracy of KNN by ensuring that all features are on a similar scale or reducing the dimensionality of the data.
  
  - 3.3 KNN Grid:
  KNN Grid is a technique that helps to determine the optimal value of K in KNN by performing a grid search. It involves training and evaluating multiple KNN models with different values of K and selecting the value that produces the best performance or accuracy on the validation set.
  
  - 3.4 Logistic Regression:
  Logistic Regression is a supervised learning algorithm used for binary classification problems. It models the relationship between the independent variables (features) and the probability of a certain outcome using the logistic function. It estimates the parameters of the logistic function using maximum likelihood estimation.
  
  - 3.5 Decision Tree:
  Decision Tree is a supervised learning algorithm that builds a tree-like model for classification or regression. It splits the data based on features at each node and makes predictions by traversing the tree from the root to the leaf nodes. It selects the best feature to split based on certain criteria such as information gain or Gini index.
  
  - 3.6 Random Forest:
  Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. It creates a set of decision trees using bootstrapped samples of the data and random feature subsets. The final prediction is made by aggregating the predictions of individual trees.
  
  - 3.7 Naive Bayes:
  Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. It assumes that the features are conditionally independent given the class label. Naive Bayes calculates the probability of each class and predicts the class with the highest probability.
  
  

```{r message=FALSE, warning=FALSE}

# Import the librarys
library (tidyverse)
library (caret)
library(psych)
library(ggplot2)
library(MASS)
library(nnet)
library(rpart)
library(rpart.plot)
library(randomForest)
library(e1071)
library(tm)
library(naivebayes)

# Obtain the current folder path and its parent folder path

folder <- dirname(rstudioapi::getSourceEditorContext()$path)
parentFolder <- dirname(folder)

#Read CSV File for trainning

Sensors <- read_csv(file = paste0(parentFolder, "/Datasets/Train_data.csv")) %>% as.data.frame()
#Read CSV File for Predict

DataTest <- read_csv(file = paste0(parentFolder, "/Datasets/Model.csv")) %>% as.data.frame()

# Give our a summary for variables Humidity,Temperatura,PPM,Room

summary(Sensors)

# Histogram of the linear model Humidity

hist(Sensors$Humidity,breaks = 100)

# Histogram of the linear model Temperature

hist(Sensors$Temperature,breaks = 100)

# Histogram of the linear model PPM

hist(Sensors$PPM,breaks = 100)

pairs.panels(Sensors[c("Humidity",
                      
                      "Temperature",
                      
                      "PPM",
                      
                      "Room")]
             
             ,pch=21, bg=c("red","green3","blue", "orange")[unclass(Sensors$Room)])

predictors <- colnames(Sensors)[-3]

sample.index <- sample(1:nrow(Sensors)
                       ,nrow(Sensors)*0.3
                       ,replace = F)

train.data <- Sensors[sample.index,c(predictors,"Room"),drop=F]
test.data <- Sensors[-sample.index,c(predictors,"Room"),drop=F]

# Use 10-Fold cross-validation for all methods

Model <-trainControl(method="cv",number=10)

# Train Model Knn without processing
Model1 <- train(Room~.,data = Sensors,method="knn",trControl=Model)
Model1

# Predict Model Knn without processing

Predict1 <- predict(Model1,newdata=DataTest,)
Predict1

# Train Model Knn with processing

Model2 <- train(Room~.,data = Sensors,method="knn",preProcess=c("center","scale"),trControl=Model)
Model2

# Predict Model Knn with processing

Predict2 <- predict(Model2,newdata =DataTest,)
Predict2

# Train Model Knn Grid

knnGrid <- expand.grid(k=c(1,5,10,30,100))
Model3 <- train(Room~.,data = Sensors,method="knn",preProcess=c("center","scale"),tuneGrid=knnGrid,trControl=Model)
Model3

# Predict model Knn Grid

Predict3 <- predict(Model3,newdata = DataTest,)
Predict3

# Train Model Logistic regression

Model4 <- multinom(Room~.,data=Sensors,iter=500)
summary(Model4)

# Predict Model Logistic regression

Predict4 <- predict(Model4,newdata=DataTest,)
Predict4
prediction<-DataTest$Predict4 <- c(Predict4)
confusionMatrix(prediction,DataTest$Predict4)

# Train Model Decision tree

CartGrid = expand.grid(maxdepth=c(1,5,10,20))
Model5 <- train(Room~.,data=Sensors,method="rpart2",trControl=Model,tuneGrid=CartGrid)
Model5
Model3$finalModel

# Predict Model Decision tree

Predict5 <- predict(Model5,newdata=DataTest,type='prob')
Predict5

# Train Model Random Forests

set.seed(2018)
Sensors$Room=factor(Sensors$Room)
randomf <-Sensors[complete.cases(Sensors),]
training.ids<-createDataPartition(Sensors$Room,p=0.7,list = F)
modrf <- randomForest(x=Sensors[training.ids,1:3],
                    y=Sensors[training.ids,4],
                    ntree =500,
                    keep.forest = TRUE)

# Predict Model Random Forest

Predict6 <- predict(modrf,newdata=DataTest,)
prediction1 <- DataTest$Predict6 <- c(Predict6)
confusionMatrix(prediction1,DataTest$Predict6)

# train Model Naivebayes

set.seed(2018)
t.idsl <- createDataPartition(Sensors$Room,p=0.7,list=F)

Model7 <- naiveBayes(Room~.,data=Sensors[t.idsl,],laplace = 1)
Model7

# Predict Model NaiveBayes

Predict7 <- predict(Model7, Sensors[-t.idsl,])
Predict7 <- predict(Model7,newdata=DataTest,)
prediction2<-DataTest$Predict7 <- c(Predict7)
confusionMatrix(prediction2,DataTest$Predict7)
```

# Conclusions :

1. KNN without preprocessing:

- KNN is a simple and intuitive algorithm that can be used for classification tasks.
- It doesn't require preprocessing, but it can be sensitive to the scale and distribution of the features.
- It can struggle with high-dimensional data or datasets with irrelevant features.
- KNN's performance heavily depends on choosing an appropriate value for K.

2. KNN with preprocessing:

- Preprocessing techniques like scaling and dimensionality reduction can improve the performance of KNN.
- Scaling ensures that all features contribute equally to the distance calculation.
- Dimensionality reduction techniques can help reduce noise and improve computational efficiency.

3. KNN Grid:

- KNN Grid allows for an automated selection of the optimal value of K.
- It involves evaluating multiple KNN models with different values of K and selecting the best performing model.
- Grid search can be computationally expensive, especially for large datasets.

4. Logistic Regression:

- Logistic Regression is a powerful algorithm for binary classification tasks.
- It assumes a linear relationship between the features and the log-odds of the target variable.
- It can handle large datasets efficiently and provides interpretable results.
- Logistic Regression performs well when the relationship between features and the target variable is roughly linear.

5. Decision Tree:

- Decision Trees are interpretable and can handle both classification and regression tasks.
- They can capture non-linear relationships between features and the target variable.
- Decision Trees are prone to overfitting, especially when the tree becomes too deep.
- Ensemble methods like Random Forest can address this issue and improve the performance.

6. Random Forest:

- Random Forest is an ensemble method that combines multiple decision trees.
- It reduces overfitting by aggregating the predictions of multiple trees.
- Random Forest provides feature importance measures, which can be helpful for feature selection.
- It can handle high-dimensional data and is generally robust to outliers and missing values.

7. Naive Bayes:

- Naive Bayes is a fast and simple algorithm that performs well on large datasets.
- It assumes feature independence, which may not hold in all cases.
- Despite the independence assumption, Naive Bayes can still provide competitive results.
- It is particularly suitable for text classification tasks and works well with high-dimensional data.













